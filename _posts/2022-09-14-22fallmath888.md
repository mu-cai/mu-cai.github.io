---
layout: post
title: Causal inference can prevent computer vision from falling into black-box deep learning
date: 2022-09-14
comments: true
tags: vision, ML, math  
description: UW-Madison 2022Fall MATH888 Project, Mu Cai
---

* TOC
{:toc}

## Homework 1

### Who am I?
My name is Mu Cai. Currently, I am a third-year Ph.D. student in the  <a href='https://cs.wisc.edu/'>Department of Computer Sciences</a>
at the <a href='https://www.wisc.edu/'>University of Wisconsin- Madison</a>, supervised by <a href='https://pages.cs.wisc.edu/~yongjaelee/'>Prof. Yong Jae Lee</a>. I got my bachelor's degree in Electrical Engineering and Automation at Xi'an Jiaotong University in 2020. My research interest lies in the intersection of deep learning and computer vision. 
I am especially interested in 3D scene understanding and self-supervised learning. You can find more information <a href='https://mu-cai.github.io/'>here</a>.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/mucai.jpeg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### What are my professional preparation/interests, background, and goals?
<!-- ### What are my professional interests or goals? -->
I am a Ph.D. student with a computer science background. I am also interested in mathematics-driven data science, 
where math (will) become my minor. Most of the time, I use deep learning, the well-known black box but powerful
tool, to empirically improve the performance of computer vision problems.
 I am passionate about solving 
**real world** problems that better serve people, such as 3D perception in autonomous driving.
However, current deep learning-based approaches may not reveal
 the underline relationship between objects and labels. For example, recent studies found that
 the irrelevant background could be an informative feature when recognizing the foreground objects,
 known as <a href="http://lgmoneda.github.io/2021/01/12/spurious-correlation-ml-and-causality.html">supurious correlation</a>.
A straightforward idea is: can we use the well-established causal inference to help improve the reliability of deep learning? 


<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/sup_cor.jpeg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>


### Why am I interested in causal inference?
Therefore, the causal inference may provide us with a way to build an interpretable 
computer vision framework. Other areas, such as vision-question-answering (VQA),
can also be equipped with causal inference to improve both final performance and 
interpretation. 


## Homework 2

### What question do you want to answer? And why is it important to answer it?

<!-- In modern computer vision,  -->
Deep learning has achieved great success in a wide spread of pattern recognition problems, including 
computer vision [[Ren et al. 2015]](#Ren_et_al_15), natural language understanding, and reinforcement. However, deep neural networks have been criticized 
as a "function approximator", i.e., it doesn't learn the true "intelligence". Instead, neural networks 
could just be a mapping from the feature to the label for the majority of samples. 
Therefore, the under-represented groups may be incorrectly predicted in a biased way.
 For example, here we consider a computer vision task called scene graph generation. We are given the objects and
 their bounding boxes. 

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/input.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

We are supposed to generate a scene graph [[Tang et al. 2020]](#Tang_et_al_20) that describes the relationship between the objects, as the 
right figure shows in the below image. However, actually, we can only get the (left) biased scene graph where 
the descriptions are vague and don't represent the accurate action or location information. 
For example, in the biased generations, there are a lot of words like "on, has, near", instead of the 
the correct descriptions like "behind, parked on".


<div class="row justify-content-sm-center">
    <div class="col-sm-0 mt-0 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/baised_prediction.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

Why could this happen? The major reason is that for the ease of annotation, most of the text descriptions 
in the training set are composed of common words like "has, on, wearing". Therefore, neural networks just 
learn such common words, and may not reflect the underrepresented actions/relationships. Therefore, it is 
important to correctly represent the scene graph using a proper inference scheme.

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/class_dist.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

Causal inference can be a great fit for this problem. Given the objects, how can we find 
the proper relationship between them? We may use the intervention model or the counterfactual causality 
to tackle this problem. We can also expand our analysis to other vision tasks like Vision-Question-Answer (VQA) [[Wang et al. 2020]](#Wang_et_al_20).
I also plan to integrate language features into the framework to improve the final performance on tasks like scene graph generation.
This approach can also improve the robustness of the interpretation ability of neural networks.

### What (observed or unobserved) random variables are needed to fully model the problem?

Here we need 4 random variables to fully model our problem. They are all observed variables under
 the current problem setting. Here $$I$$ denotes the image, which can be represented as the high-level
  semantic features from a feature extractor backbone. $$X$$ denotes the object features, which are also 
 the feature maps in the deep neural network. $$Z$$ denotes the object labels. $$Y$$ denotes the predicated logits.



<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/graph.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

Here is a concrete example, where the nodes and links in DAG are clearly presented. 

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> -->
  <!-- <div class="row">
    <div class="col one"> -->
        {% include figure.html path="assets/img/model_varaible.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>



### What is the causal effect that you wish to study?
Here I want to study the Total Direct Effect [[VanderWeele et al. 2020]](#VanderWeele_et_al_13) [[Tang et al. 2020]](#Tang_et_al_20). 

$$
T D E=Y_x(i)-Y_{\bar{x}, z}(i),
$$

where $$ Y_x(i) $$ is the observed outcome. Here I would like to use counterfactual causality based on the intervention $$X=\bar{x}$$ to conduct the analysis. 
Thus the first term is from the original graph and the second
one is from the counterfactual. Within this process, do-calculus is also introduced, e.g., $$do(X=\bar{x})$$. 

### What are your working hypotheses about the relationship between variables?

Here I am also working with DAG. From my problem setting, we know that all edges are directed, and there are no  cycles. 
Therefore, we can say that it has a topological or causal ordering. Three assumptions in the potential outcome framework can also be applied here. 


---
### References
---
- <a name="Ren_et_al_15"></a> **\[Ren et al. 2015\]**  Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun "*[Faster r-cnn: Towards real-time object detection with region proposal networks](https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)*", Advances in neural information processing systems 28 (2015)
- <a name="Wang_et_al_20"></a> **\[Wang et al. 2020\]**  Wang, Tan, Jianqiang Huang, Hanwang Zhang, and Qianru Sun "*[Visual commonsense r-cnn](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Visual_Commonsense_R-CNN_CVPR_2020_paper.pdf)*", In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10760-10770. 2020
- <a name="Tang_et_al_20"></a> **\[Tang et al. 2015\]**   Tang, Kaihua, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang"*[Unbiased scene graph generation from biased training](https://arxiv.org/abs/2002.11949)*", In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3716-3725. 2020
- <a name="VanderWeele_et_al_13"></a> **\[VanderWeele et al. 2013\]**   VanderWeele, Tyler J "*[A three-way decomposition of a total effect into direct, indirect, and interactive effects](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563853/)*",Epidemiology (Cambridge, Mass.) 24, no. 2 (2013): 224.